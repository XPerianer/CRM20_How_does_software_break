{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import git\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from git import Repo\n",
    "from unidiff import PatchSet\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from src import loading\n",
    "from src import predictor\n",
    "from src import preprocessing\n",
    "from src import visualization\n",
    "from src.reorderer import *\n",
    "from src.reordering_analyzer import ReorderingAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure\n",
    "\n",
    "This notebook is seperated into 3 main sections\n",
    "\n",
    "## 1. [Visualization](#Visualization)\n",
    "    \n",
    "Making the mutation testing data gathered visible and exploring the test suites visually.\n",
    "\n",
    "## 2. [Prediction](#Prediction)\n",
    "\n",
    "Preparing the data for predicting failing tests, and exploring features and different models.\n",
    "\n",
    "## 3. [Reordering](#Reordering)\n",
    "\n",
    "Using the prepared data for a dive into Test Case Reordering, using both binary classifiers from before, as other methods and comparing there results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What data are we working with?\n",
    "\n",
    "Our starting point is mutation testing data generated by [Mutester](https://github.com/XPerianer/CRM2020), a tool that was developed for exactly this task.\n",
    "Also have a look at the [midterm presentation slides](../presentation/midterm.pdf) for more information about how and why we gathered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to use the sparsify flag, if you want to test if models throw errors / for a quick check.\n",
    "# USE WITH CAUTION: This is by no means designed to give an appropriate / random sample.\n",
    "# For long running cells, I also tried to split the visualization with the resource intense part, so the notebook can run overnight, and then the output will be generated when you reconnect.\n",
    "datasets = loading.load_datasets({\n",
    "    'Flask': 'data/flask_full.pkl',\n",
    "    'Jinja': 'data/jinja_full.pkl',\n",
    "    'Httpie': 'data/httpie_full.pkl',\n",
    "    'Docopt': 'data/docopt_full.pkl'\n",
    "}, sparsify=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main structure of such a datasets is that for each mutant, with it's respective `mutant_id`, and every test case, with it's respective `test_id`,\n",
    "there is a row in the dataset, recording the outcome of the test, and additional information, like how long the test took, what files where touched, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "## Visualising Data generated by Mutation testing\n",
    "\n",
    "In the beginning of the project, I tried to get a glimpse of the generated data by trying out different visualizations,\n",
    "the most basic beeing the covariance matrix, and then two different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, mutants_and_tests in datasets.items():\n",
    "    visualization.plot_covariance_matrix(name, mutants_and_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting aspect is also how often tests fail in general, and how the distribution is through the datasets:\n",
    "\n",
    "What we can observe is that while flask and httpie have a very low number of often failing tests, jinja and docopt have a wider variety of tests failing.\n",
    "We can see the behavoior also in the covariance matrixes, that are a lot lighter for flask and httpie, due to less covariance in tests often failing together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in datasets.items():\n",
    "    visualization.plot_failure_histogram(name,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, it will also be interesting how the failure of the test correlates to the average duration of the test. You can see this in these scatter plots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in datasets.items():\n",
    "    visualization.plot_failures_vs_duration(name,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Plot of tests\n",
    "\n",
    "\n",
    "To try to get even more insight into the different datasets, I tried to come up with my own domain specific visualization, that builds on top of the failure histogram.\n",
    "\n",
    "We print one dot for every test, with the following properties:\n",
    "x axis = average of ids of changed line (mutant_ids) that make this test fail\n",
    "y axis = number of mutants that make this test fail\n",
    "\n",
    "So for example a unit test should be relatively wide down, since it should not fail on too many mutants.\n",
    "\n",
    "To show the 'spread' of which mutant_ids made which tests fail, we draw an arrow from (mutant_id, 0) to (x,y) of the test, iff mutant_id makes the test fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For larger datasets, this may take a while (~5minutes). You can disable the arrows, then it becomes seconds\n",
    "for name, mutants_and_tests in datasets.items():\n",
    "    visualization.plot_hierarchical_failures(name, mutants_and_tests, arrows=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "## Predicting failing tests\n",
    "\n",
    "After having a fist visualization the data (and we will continue to do that), the question is if we can train machine learning models to predict failing tests.\n",
    "While this has obious applications in test selection and reordering, we might also be able to find hidden dependecies in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing and Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_column_names = [\"modified_method\", \"modified_file_path\", \"name\", \"filepath\", \"current_line\", \"previous_line\"]\n",
    "\n",
    "encoded_datasets = {}\n",
    "for name, mutants_and_tests in datasets.items():\n",
    "    preprocessing.cleanse_data(mutants_and_tests)\n",
    "    preprocessing.add_edit_distance_feature(mutants_and_tests)\n",
    "    mutants_and_tests = preprocessing.filter_NaN_values(name, mutants_and_tests)\n",
    "    encoded_datasets[name] = preprocessing.encode_columns(mutants_and_tests, encoded_column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method_name feature seems to make sense from a human perspective: If I change code in the same method, I would expect similar tests to fail.\n",
    "But to go even further, I tried to add a numerical value, the edit distance between the test name and the method name. The expectation here was, that when both are similar, the test is more likely to fail. You can see examples for that in the [presentation slides, page 40](../presentation/final.pdf)\n",
    "\n",
    "In the ROC Curve, we can see that if we use the edit_distance as a standalone feature, Httpie and Flask behave better than Jinja and Docopt.\n",
    "This is interesting, since we have previously seen that both of these projects also had a better distinsting between 'Unit Tests' that test a very specific part of the code, and very few integration tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_edit_distance_roc_curve(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "We can now select the features we want to use for our predictions.\n",
    "\n",
    "To be on the save side, we previously drop\n",
    "* features that could spoil y (`dangerous_features`)\n",
    "* features that are unencoded (`unencoded_features`)\n",
    "\n",
    "The features are grouped in three categories:\n",
    "* Basic features, that are generated with the diff of the mutant, and also test properties\n",
    "* Semantic features, which are generated from the semantics in the mutant\n",
    "* Context features, which are gathered in the code around the test and the mutant\n",
    "See also the [final_presentation_slides, page 37pp](../presentation/final.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dangerous_features = ['duration', 'setup_outcome', 'setup_duration', 'call_outcome', 'call_duration', 'teardown_outcome', 'teardown_duration']\n",
    "unencoded_features = ['repo_path', 'full_name']\n",
    "\n",
    "selected_features = [\n",
    "    # Basic features:\n",
    "    'current_line', 'line_number_changed',\n",
    "    'modified_file_path', 'previous_line', 'test_id',\n",
    "    'name', 'filepath', 'mutant_id', 'outcome',\n",
    "    # Semantic features:\n",
    "    'modified_method', 'edit_distance',\n",
    "    # Context features:\n",
    "    'contains_branch_mutant',\n",
    "    'contains_equality_comparison_mutant', 'contains_loop_mutant',\n",
    "    'contains_math_operands_mutant', 'contains_loop_execution', 'contains_math_operands_execution',\n",
    "    'contains_equality_comparison_execution', 'contains_branch_execution'\n",
    "]\n",
    "    \n",
    "\n",
    "for name, mutants_and_tests in encoded_datasets.items():\n",
    "    encoded_datasets[name] = mutants_and_tests.drop(dangerous_features, axis=1).drop(unencoded_features, axis=1)[selected_features].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_data = {}\n",
    "\n",
    "for name, mutants_and_tests in encoded_datasets.items():\n",
    "    test_train_data[name] = preprocessing.train_test_split(mutants_and_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training here takes a long time, depending on the model it can go over hours\n",
    "predictors = {\n",
    "    'NearestMutant': predictor.NearestMutantPredictor(),\n",
    "    'KNeighbors': KNeighborsClassifier(n_neighbors=1),\n",
    "    'DecisionTree': tree.DecisionTreeClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(random_state=420),\n",
    "    'GradientBoosing': GradientBoostingClassifier(random_state=420),\n",
    "}\n",
    "\n",
    "fitted_predictors = {}\n",
    "\n",
    "for dataset_name, mutants_and_tests in encoded_datasets.items():\n",
    "    X_train, y_train, X_test, y_test = test_train_data[dataset_name]\n",
    "    fitted_predictors[dataset_name] = {}\n",
    "    for predictor_name, predictor_instance in predictors.items():\n",
    "        predictor_instance = copy.deepcopy(predictor_instance)\n",
    "        predictor_instance.fit(X_train, y_train)\n",
    "        fitted_predictors[dataset_name][predictor_name] = predictor_instance\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitted visualization\n",
    "for dataset_name, mutants_and_tests in encoded_datasets.items():\n",
    "    X_train, y_train, X_test, y_test = test_train_data[dataset_name]\n",
    "    for predictor_name, predictor_instance in fitted_predictors[dataset_name].items():\n",
    "        visualization.plot_confusion_matrix(dataset_name + ' ' + predictor_name, predictor_instance, X_test, y_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are especially interested in the \"True False Predicted False\" score, as we can use that to make a good reordering for example. False positives are not too bad in this scenario.\n",
    "While we see that especially on the flask dataset the RandomForestClassifier performs good, this does not generalize to all datasets.\n",
    "One nice thing about this classifier is that we can try to debug it using it's feature importances. These can give us a hint on what the forest uses to decide between a false and true outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, fitted_predictor in fitted_predictors.items():\n",
    "    visualization.plot_feature_importances(name, fitted_predictor['RandomForest'], test_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that for the flask dataset, the random forest makes more usage of the 'semantic' features in the dataset, which might also be an indication that the tests in that project are especially well organized.\n",
    "It would be an interesting point of research to check wether this can also be perceived by the human as a 'predictable' test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reordering\n",
    "\n",
    "## Reordering the testcases to improve the developer experience\n",
    "\n",
    "While it is nice to be able to predict which test fail and which not, it is easier to decide if a classification is good or bad based on a concrete use case.\n",
    "Therefore, the predictors are tested on various metrics in the test case reordering domain.\n",
    "* APFD, which is a measure on how good a order of tests is in general, but which is duration agnostic\n",
    "* APFDc, which is APFDc weighted after the average durations of the mentioned tests\n",
    "* First Failing, which is the time that emerges till a test is found that fails\n",
    "* Last Failing, which is the time that emerges till the last test that fails is found\n",
    "\n",
    "There are multiple ways to derive an order for test case execution. To bridge the gap between prediction and reordering, there are 2 main helper classes:\n",
    "* `BinaryPredictionReorderer`, which takes a binary predictor, and moves the (predicted) failing tests to the front\n",
    "* `OrdinaryPredictionReorderer`, which sorts the tests ascending after there (predicted) chance of failing\n",
    "\n",
    "To have some baseline, there are also some mutant agnostic Reorderes. That means that they will output the same order for every mutant:\n",
    "* `NaiveReorderer`, which just outputs the tests after ascending test_ids\n",
    "* `AverageReorderer`, which orders the tests after there overall a priori probability to fail\n",
    "* `QTF`, which is sorting the tests ascending after there duration. It is a heuristic mentioned in [this paper](https://dl.acm.org/doi/pdf/10.1145/3395363.3397383)\n",
    "\n",
    "The `ReorderingAnalyzer` class can be used to automatically train and evaluate the reorderes, and print boxplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step takes not only long to train, but also long to evaluate, with a lot of predictors it will take hours. Every 50 orders processed there will be a dot printed,\n",
    "# so you can get a feelint on how long it will take.\n",
    "\n",
    "evaluations = {}\n",
    "for name, mutants_and_tests in encoded_datasets.items():\n",
    "    evaluation = ReorderingAnalyzer([\n",
    "        NaiveReorderer(),\n",
    "        AverageReorderer(),\n",
    "        QTF(datasets[name][['test_id', 'duration']]),\n",
    "        BinaryPredictionReorderer(tree.DecisionTreeClassifier()),\n",
    "        BinaryPredictionReorderer(KNeighborsClassifier()),\n",
    "        BinaryPredictionReorderer(RandomForestClassifier(random_state=420)),\n",
    "        OrdinalPredictionReorderer(RandomForestClassifier(random_state=420)),\n",
    "        BinaryPredictionReorderer(GradientBoostingClassifier(random_state=420)),\n",
    "        OrdinalPredictionReorderer(GradientBoostingClassifier(random_state=420))\n",
    "    ])\n",
    "    X_train, y_train, X_test, y_test = preprocessing.train_test_split(mutants_and_tests)\n",
    "    evaluation.fit(X_train, y_train)\n",
    "    evaluation.predict(X_test)\n",
    "    evaluation_data = evaluation.evaluate(datasets[name])\n",
    "    evaluations[name] = evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, evaluation in evaluations.items():\n",
    "    print('Dataset: ' + name)\n",
    "    evaluation.boxplot()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (other-env)",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
